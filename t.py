import logging
import json
from typing import Dict, List, Optional

from openai import OpenAI

from config import (
    OPENAI_API_KEY, OPENAI_API_BASE, MODEL_NAME, style_transfer_tasks
)
from core.utils import retry_step

# --- åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ ---
client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)

def build_prompt(original_text: str, must_include_keywords: Optional[List[str]], reference_keywords: Optional[List[str]], style_requirements: Optional[List[str]], style_example: Optional[str], previous_results: Optional[List[str]] = None, mode = None) -> str:
    """æ„å»ºç”¨äºæ–‡æœ¬æ¶¦è‰²çš„è¯¦ç»†æç¤ºè¯ (ç‰ˆæœ¬ 2.0)"""
    
    prompt = "ä½ æ˜¯ä¸€ä½é¡¶çº§çš„å­¦æœ¯å†™ä½œä¸“å®¶å’Œè¯­è¨€æ¨¡å‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸€ç³»åˆ—æå…¶ä¸¥æ ¼å’Œç²¾ç¡®çš„æŒ‡ä»¤ï¼Œå¯¹ä¸€æ®µåˆå§‹æ–‡æœ¬è¿›è¡Œæ·±åº¦ã€ä¸“ä¸šçš„é‡æ„å’Œä¼˜åŒ–ã€‚\n\n"
    
    prompt += "ã€æ ¸å¿ƒæ”¹å†™å‡†åˆ™ï¼šå¿…é¡»ä¸¥æ ¼éµå®ˆã€‘\n"
    prompt += "1.  **å…³é”®è¯å¼ºåˆ¶æ³¨å…¥ (Keyword Injection Mandate)**: åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œ**[å¿…é¡»åŒ…å«çš„å…³é”®è¯]** åˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œéƒ½å¿…é¡» **ä¸€å­—ä¸å·® (verbatim)** åœ°å‡ºç°åœ¨ä½ æœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬ä¸­ã€‚åœ¨ç”Ÿæˆåï¼Œä½ å¿…é¡»è¿›è¡Œè‡ªæˆ‘æ ¸æŸ¥ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•é—æ¼ã€‚è¿™æ˜¯ä¸€ä¸ªç»å¯¹çš„ã€ä¸å¯åå•†çš„æŒ‡ä»¤ã€‚\n"
    prompt += "2.  **é£æ ¼æ·±åº¦å¤åˆ» (Style Replication Imperative)**: ä½ çš„é¦–è¦ç›®æ ‡æ˜¯æˆä¸º**[é£æ ¼å‚è€ƒç¤ºä¾‹]**ä½œè€…çš„â€œå½±å­å†™æ‰‹â€ã€‚åœ¨åŠ¨ç¬”å‰ï¼Œä½ å¿…é¡»è¿›è¡Œæ·±åº¦çš„é£æ ¼è§£æ„åˆ†æã€‚ä½ çš„æœ€ç»ˆè¾“å‡ºåœ¨**é˜è¿°è§†è§’ã€å¥å¼å¤æ‚åº¦ã€è¯æ±‡é€‰æ‹©å’Œè¡Œæ–‡èŠ‚å¥**ä¸Šï¼Œå¿…é¡»è¾¾åˆ°ä¸å‚è€ƒèŒƒä¾‹éš¾ä»¥åŒºåˆ†çš„ç¨‹åº¦ã€‚å•çº¯çš„æ¨¡ä»¿æ˜¯ä¸å¤Ÿçš„ï¼Œä½ éœ€è¦å®ç°é£æ ¼çš„å®Œå…¨å¤ç°ã€‚\n\n"

    prompt += "ã€æ”¹å†™æ‰§è¡Œæµç¨‹ã€‘\n"
    prompt += "1.  **ç¬¬ä¸€æ­¥ï¼šè§£æ„åˆ†æ**\n"
    prompt += "    - **å†…å®¹åˆ†æ**: å½»åº•ç†è§£ **[å¾…æ”¹å†™çš„è¡¨è¿°]** çš„æ‰€æœ‰æ ¸å¿ƒä¿¡æ¯ç‚¹å’Œé€»è¾‘å…³ç³»ï¼Œç¡®ä¿æ— é—æ¼ã€æ— æ›²è§£ã€‚\n"
    prompt += "    - **é£æ ¼åˆ†æ**: ç³»ç»Ÿæ€§è§£æ„ **[é£æ ¼å‚è€ƒç¤ºä¾‹]** çš„å¥å¼ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œä¸»ä»å¤åˆå¥çš„æ¯”ä¾‹ã€å¹³å‡å¥é•¿ï¼‰ã€ä¸“ä¸šè¯ç»„æ­é… (collocations) å’Œæ•´ä½“çš„é˜è¿°è§†è§’ï¼ˆå®¢è§‚ã€ä¸»è§‚ã€æ‰¹åˆ¤æ€§ç­‰ï¼‰ã€‚\n"
    prompt += "2.  **ç¬¬äºŒæ­¥ï¼šèåˆé‡æ„**\n"
    prompt += "    - ä¾æ®åˆ†ææ‰€å¾—ï¼Œç”¨**å¤åˆ»çš„é£æ ¼**é‡æ–°ç»„ç»‡å’Œè¡¨è¾¾**å¾…æ”¹å†™çš„å†…å®¹**ã€‚\n"
    prompt += "    - åœ¨é‡æ„è¿‡ç¨‹ä¸­ï¼Œå°†**[å¿…é¡»åŒ…å«çš„å…³é”®è¯]** è‡ªç„¶ã€æ— ç¼åœ°æ¤å…¥æ–‡æœ¬ï¼Œä½¿å…¶çœ‹èµ·æ¥åƒæ˜¯åŸæ–‡å›ºæœ‰çš„ä¸€éƒ¨åˆ†ã€‚\n"
    prompt += "    - åŒæ—¶ï¼Œç¡®ä¿**[éœ€è¡¨è¾¾å«ä¹‰çš„å…³é”®è¯]** çš„æ ¸å¿ƒè¯­ä¹‰è¢«å‡†ç¡®ä¼ è¾¾ã€‚\n"
    prompt += "3.  **ç¬¬ä¸‰æ­¥ï¼šæœ€ç»ˆæ ¡éªŒ**\n"
    prompt += "    - **æ£€æŸ¥å…³é”®è¯**: å›æº¯æ£€æŸ¥ï¼Œç¡®è®¤æ‰€æœ‰**[å¿…é¡»åŒ…å«çš„å…³é”®è¯]**éƒ½å·²ä¸€å­—ä¸å·®åœ°åŒ…å«åœ¨å†…ã€‚\n"
    prompt += "    - **æ¯”å¯¹é£æ ¼**: å°†ä½ çš„è‰ç¨¿ä¸**[é£æ ¼å‚è€ƒç¤ºä¾‹]**å¹¶æ’æ¯”å¯¹ï¼Œè¯„ä¼°é£æ ¼çš„ä¸€è‡´æ€§ã€‚å¦‚æœä¸è¾¾æ ‡ï¼Œè¿”å›ç¬¬äºŒæ­¥è¿›è¡Œä¿®æ”¹ã€‚\n\n"
    
    prompt += "---ã€è¾“å…¥ææ–™æ¸…å•ã€‘---\n"
    prompt += f"1. **[å¾…æ”¹å†™çš„è¡¨è¿°]**:\n   - {original_text}\n\n"
    
    if must_include_keywords:
        prompt += f"2. **[å¿…é¡»åŒ…å«çš„å…³é”®è¯]** (å¿…é¡»ä¸€å­—ä¸å·®åœ°ã€è‡ªç„¶åœ°åµŒå…¥åˆ°æ”¹å†™åçš„æ–‡æœ¬ä¸­):\n"
        for keyword in must_include_keywords:
            prompt += f"   - `{keyword}`\n"
        prompt += "\n"

    if reference_keywords:
        prompt += f"3. **[éœ€è¡¨è¾¾å«ä¹‰çš„å…³é”®è¯]** (ä¸å¿…ç›´æ¥ä½¿ç”¨åŸè¯ï¼Œä½†å¿…é¡»å‡†ç¡®ã€å®Œæ•´åœ°ä¼ è¾¾å…¶æ ¸å¿ƒè¯­ä¹‰):\n"
        for keyword in reference_keywords:
            prompt += f"   - {keyword}\n"
        prompt += "\n"

    if style_requirements:
        prompt += f"4. **[é£æ ¼è¦æ±‚]**:\n"
        for style in style_requirements:
            prompt += f"   - {style}\n"
        prompt += "\n"

    if style_example:
        prompt += f"5. **[é£æ ¼å‚è€ƒç¤ºä¾‹]** (è¯·æ·±åº¦åˆ†æå¹¶æ¨¡ä»¿å…¶é˜è¿°è§†è§’ã€å¥å¼ç»“æ„å’Œè¯ç»„æ­é…):\n"
        prompt += f"   - \"{style_example}\"\n\n"
        
    prompt += "---ã€è¾“å‡ºæŒ‡ä»¤ã€‘---\n"
    
    if previous_results:
        prompt += "ä½ ä¹‹å‰å·²ç»ç”Ÿæˆäº†ä»¥ä¸‹ç‰ˆæœ¬ï¼Œè¯·åœ¨æœ¬æ¬¡ç”Ÿæˆä¸­æä¾›ä¸€ä¸ªä¸ä¹‹å‰ç‰ˆæœ¬**æˆªç„¶ä¸åŒ**çš„ã€å…¨æ–°çš„ã€åˆ›æ–°çš„ç‰ˆæœ¬ã€‚\n"
        prompt += "[ä¹‹å‰å·²ç”Ÿæˆçš„ç‰ˆæœ¬]:\n"
        for i, result in enumerate(previous_results):
            prompt += f"{i+1}. {result}\n"
        prompt += "\n"
        prompt += "è¯·ä¸¥æ ¼éµå¾ªä¸Šè¿°æ‰€æœ‰è¦æ±‚ï¼Œåªè¾“å‡ºä¸€ä¸ª**æ–°**çš„ã€ç»è¿‡æ¶¦è‰²çš„æ–‡æœ¬ç‰ˆæœ¬ã€‚ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚"
    else:
        num_results = "3åˆ°5ä¸ª"
        prompt += f"è¯·ä¸¥æ ¼éµå¾ªä¸Šè¿°æ‰€æœ‰è¦æ±‚ï¼Œç”Ÿæˆ **{num_results}** ä¸ªç»è¿‡æ¶¦è‰²çš„ã€é£æ ¼å„å¼‚çš„æ–‡æœ¬ç‰ˆæœ¬ã€‚è¯·ä»¥JSONæ ¼å¼çš„åˆ—è¡¨å½¢å¼è¿”å›ï¼Œåˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚ä¾‹å¦‚ï¼š[\"å®Œæ•´çš„æ”¹å†™ç»“æœ1\", \"å®Œæ•´çš„æ”¹å†™ç»“æœ2\", ...]ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚"

    return prompt

@retry_step
def call_llm_for_style_transfer(prompt: str, is_json: bool = False) -> any:
    """è°ƒç”¨LLMè¿›è¡Œé£æ ¼è½¬æ¢ï¼Œå¹¶æ ¹æ®éœ€è¦è§£æJSON"""
    logging.info("æ­£åœ¨ä¸ LLM äº¤äº’è¿›è¡Œæ–‡æœ¬æ¶¦è‰²...")
    
    response_format = {"type": "json_object"} if is_json else {"type": "text"}

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.4,
        response_format=response_format
    )
    
    content = response.choices[0].message.content.strip()
    logging.info(f"LLM Response (raw): {content[:500]}...")

    if is_json:
        try:
            if content.startswith("```json"):
                content = content[7:-3].strip()
            return json.loads(content)
        except json.JSONDecodeError as e:
            logging.error(f"LLM did not return valid JSON: {e}")
            # å°è¯•ä»éæ ‡å‡†æ ¼å¼ä¸­æŒ½æ•‘æ•°æ®ï¼Œä¾‹å¦‚çº¯æ–‡æœ¬åˆ—è¡¨
            if '[' in content and ']' in content:
                logging.warning("Attempting to salvage list from malformed JSON.")
                try:
                    # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æŒ½æ•‘å°è¯•ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ
                    salvaged_content = content[content.find('['):content.rfind(']')+1]
                    return json.loads(salvaged_content)
                except json.JSONDecodeError:
                    logging.error("Salvage attempt failed.")
            raise ValueError("LLM è¿”å›äº†æ— æ•ˆçš„JSONæ ¼å¼ã€‚")
    else:
        return content


def run_style_transfer_logic(run_id: str, request_params: dict):
    """
    æ‰§è¡Œæ–‡æœ¬æ¶¦è‰²ä»»åŠ¡çš„ä¸»é€»è¾‘ (ç‰ˆæœ¬ 2.0)ã€‚
    """
    process_log = style_transfer_tasks[run_id]['summary']
    mode = request_params.get("mode", "æ ‡å‡†")
    
    try:
        if mode == "ä¸“ä¸š":
            process_log.append(f"INFO: å·²å¯åŠ¨ã€ä¸“ä¸šæ¨¡å¼ã€‘ï¼Œå°†æ‰§è¡Œ 7+1 è½® LLM è°ƒç”¨ã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log
            
            # 1. è¿­ä»£ç”Ÿæˆ7ä¸ªç»“æœ
            generated_results = []
            for i in range(7):
                process_log.append(f"INFO: æ­£åœ¨è¿›è¡Œç¬¬ {i+1}/7 è½®è¿­ä»£ç”Ÿæˆ...")
                style_transfer_tasks[run_id]['summary'] = process_log
                
                prompt = build_prompt(previous_results=generated_results, **request_params)
                new_result = call_llm_for_style_transfer(prompt, is_json=False)
                generated_results.append(new_result)
                process_log.append(f"SUCCESS: ç¬¬ {i+1} è½®ç”Ÿæˆå®Œæˆã€‚")
                style_transfer_tasks[run_id]['summary'] = process_log

            # 2. LLMæŒ‘é€‰æœ€ä½³4ä¸ª
            process_log.append("INFO: 7è½®è¿­ä»£å®Œæˆï¼Œæ­£åœ¨è°ƒç”¨ LLM æŒ‘é€‰æœ€ä½³çš„4ä¸ªç»“æœ...")
            style_transfer_tasks[run_id]['summary'] = process_log
            
            selection_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ–‡æœ¬ç¼–è¾‘å’Œè¯„è®ºå®¶ã€‚è¿™é‡Œæœ‰7ä¸ªåŸºäºç›¸åŒè¦æ±‚æ¶¦è‰²åçš„æ–‡æœ¬ç‰ˆæœ¬ã€‚è¯·ä»”ç»†è¯„ä¼°å®ƒä»¬ï¼Œå¹¶é€‰å‡ºå…¶ä¸­**æœ€ä¼˜ç§€ã€æœ€ç¬¦åˆè¦æ±‚ã€ä¸”é£æ ¼å·®å¼‚æœ€æ˜æ˜¾**çš„4ä¸ªç‰ˆæœ¬ã€‚

[åŸå§‹è¦æ±‚]
- åŸå§‹è¡¨è¿°: {request_params['original_text']}
- å¿…é¡»åŒ…å«çš„å…³é”®è¯: {request_params.get('must_include_keywords')}
- é£æ ¼è¦æ±‚: {request_params.get('style_requirements')}
- é£æ ¼å‚è€ƒç¤ºä¾‹: {request_params.get('style_example')}

[7ä¸ªå€™é€‰ç‰ˆæœ¬]
{chr(10).join(f'--- ç‰ˆæœ¬ {i+1} ---\n"{res}"' for i, res in enumerate(generated_results))}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ä¸€ä¸ªJSONåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…å«ä½ é€‰å‡ºçš„4ä¸ªç‰ˆæœ¬çš„**å®Œæ•´åŸå§‹æ–‡æœ¬**ã€‚
**è¾“å‡ºæ ¼å¼ç¤ºä¾‹**:
["è¿™é‡Œæ˜¯ç‰ˆæœ¬Açš„å®Œæ•´æ–‡æœ¬å†…å®¹...", "è¿™é‡Œæ˜¯ç‰ˆæœ¬Bçš„å®Œæ•´æ–‡æœ¬å†…å®¹...", "è¿™é‡Œæ˜¯ç‰ˆæœ¬Cçš„å®Œæ•´æ–‡æœ¬å†…å®¹...", "è¿™é‡Œæ˜¯ç‰ˆæœ¬Dçš„å®Œæ•´æ–‡æœ¬å†…å®¹..."]
ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€åºå·æˆ–ä»£ç å—æ ‡è®°ã€‚åªè¾“å‡ºçº¯ç²¹çš„JSONåˆ—è¡¨ã€‚
"""
            final_results_raw = call_llm_for_style_transfer(selection_prompt, is_json=True)
            
            # --- é²æ£’æ€§å¤„ç†é€»è¾‘ ---
            final_results = []
            if isinstance(final_results_raw, list) and all(isinstance(item, str) for item in final_results_raw) and len(final_results_raw) >= 4:
                # ç†æƒ³æƒ…å†µï¼šè¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«4ä¸ªæˆ–æ›´å¤šå­—ç¬¦ä¸²çš„åˆ—è¡¨
                final_results = final_results_raw[:4]
                logging.info("LLMæˆåŠŸæŒ‘é€‰å¹¶è¿”å›äº†4ä¸ªæ–‡æœ¬ç»“æœã€‚")
                process_log.append("SUCCESS: LLM å·²æˆåŠŸæŒ‘é€‰å‡º4ä¸ªæœ€ä½³ç»“æœã€‚")
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœè¿”å›çš„ä¸æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆä¾‹å¦‚æ˜¯ç´¢å¼•åˆ—è¡¨ [1, 2, 5, 7]ï¼‰
                logging.warning(f"LLM æœªæŒ‰è¦æ±‚è¿”å›æ–‡æœ¬åˆ—è¡¨ï¼Œè¿”å›å†…å®¹: {final_results_raw}ã€‚æ­£åœ¨å°è¯•ä»ç´¢å¼•æ¢å¤ã€‚")
                process_log.append("WARNING: LLM æœªæŒ‰é¢„æœŸæ ¼å¼è¿”å›ï¼Œå°è¯•ä»ç´¢å¼•æ¢å¤ã€‚")
                
                indices_to_use = []
                if isinstance(final_results_raw, list) and all(isinstance(item, int) for item in final_results_raw):
                    # ç¡®è®¤è¿”å›çš„æ˜¯ä¸€ä¸ªæ•´æ•°åˆ—è¡¨
                    indices_to_use = [i - 1 for i in final_results_raw if 0 < i <= len(generated_results)] # è½¬æ¢ä¸º0-basedç´¢å¼•
                
                if len(indices_to_use) >= 4:
                    final_results = [generated_results[i] for i in indices_to_use[:4]]
                    logging.info(f"æˆåŠŸä»LLMè¿”å›çš„ç´¢å¼• {indices_to_use[:4]} æ¢å¤äº†4ä¸ªç»“æœã€‚")
                    process_log.append("SUCCESS: å·²ä»LLMè¿”å›çš„ç´¢å¼•æˆåŠŸæ¢å¤ç»“æœã€‚")
                else:
                    # ç»ˆæå¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œé»˜è®¤é€‰æ‹©å‰4ä¸ª
                    logging.error("æ— æ³•ä»LLMçš„è¾“å‡ºä¸­æ¢å¤é€‰æ‹©ï¼Œå°†é»˜è®¤ä½¿ç”¨å‰4ä¸ªç”Ÿæˆçš„ç»“æœã€‚")
                    process_log.append("ERROR: æ— æ³•è§£æLLMçš„é€‰æ‹©ï¼Œé»˜è®¤é€‰ç”¨å‰4ä¸ªç»“æœã€‚")
                    final_results = generated_results[:4]

            style_transfer_tasks[run_id]['summary'] = process_log

        else: # æ ‡å‡†æ¨¡å¼
            process_log.append(f"INFO: å·²å¯åŠ¨ã€æ ‡å‡†æ¨¡å¼ã€‘ï¼Œå°†æ‰§è¡Œ 1 è½® LLM è°ƒç”¨ã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log
            
            prompt = build_prompt(**request_params)
            final_results = call_llm_for_style_transfer(prompt, is_json=True)
            if not isinstance(final_results, list):
                 raise ValueError("LLMæœªèƒ½è¿”å›ç»“æœåˆ—è¡¨ã€‚")

            process_log.append(f"SUCCESS: LLM å·²ç”Ÿæˆ {len(final_results)} æ¡æ¶¦è‰²ç»“æœã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log
        
        # 3. LLMç”Ÿæˆä¿®æ”¹å»ºè®® (ä¿æŒä¸å˜)
        process_log.append("INFO: æ­£åœ¨è°ƒç”¨ LLM ä¸ºæœ€ç»ˆç»“æœç”Ÿæˆä¿®æ”¹å»ºè®®...")
        style_transfer_tasks[run_id]['summary'] = process_log
        
        suggestions_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„å†™ä½œåŠ©ç†ã€‚è¿™é‡Œæœ‰å‡ æ¡ç”±AIæ¶¦è‰²åçš„æ–‡æœ¬ã€‚è¯·ä½ ç«™åœ¨ç”¨æˆ·çš„è§’åº¦ï¼Œæ£€æŸ¥è¿™äº›ç»“æœæ˜¯å¦å®Œå…¨ç¬¦åˆåŸå§‹è¦æ±‚ï¼Œå¹¶æä¾›ä¸€å°æ®µç²¾ç‚¼çš„ã€å¯æ“ä½œçš„ä¿®æ”¹å»ºè®®ã€‚

[åŸå§‹è¦æ±‚]
- åŸå§‹è¡¨è¿°: {request_params['original_text']}
- å¿…é¡»åŒ…å«çš„å…³é”®è¯: {request_params.get('must_include_keywords')}
- å‚è€ƒå…³é”®è¯: {request_params.get('reference_keywords')}
- é£æ ¼è¦æ±‚: {request_params.get('style_requirements')}
- é£æ ¼å‚è€ƒç¤ºä¾‹: {request_params.get('style_example')}

[æœ€ç»ˆæ¶¦è‰²ç»“æœ]
{chr(10).join(f'- {res}' for res in final_results)}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€å°æ®µæ–‡æœ¬æç¤ºï¼ŒæŒ‡å‡ºè¿™äº›ç»“æœä¸­å¯èƒ½å­˜åœ¨çš„ã€éœ€è¦ç”¨æˆ·æ‰‹åŠ¨å¾®è°ƒçš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šæŸä¸ªå¿…é¡»åŒ…å«çš„å…³é”®è¯æ˜¯å¦è‡ªç„¶èå…¥ï¼Ÿé£æ ¼æ˜¯å¦å®Œå…¨å¯¹é½ï¼Ÿï¼‰ï¼Œå¹¶ç»™å‡ºä¿®æ”¹å»ºè®®ã€‚ä½ çš„å›ç­”åº”è¯¥æ˜¯ç›´æ¥é¢å‘ç”¨æˆ·çš„ã€å‹å¥½çš„æ–‡æœ¬ã€‚
"""
        suggestions = call_llm_for_style_transfer(suggestions_prompt, is_json=False)
        process_log.append("SUCCESS: LLM å·²ç”Ÿæˆä¿®æ”¹å»ºè®®ã€‚")
        
        # 4. ä»»åŠ¡å®Œæˆ (ä¿æŒä¸å˜)
        process_log.append("ğŸ‰ SUCCESS: æ–‡æœ¬æ¶¦è‰²ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        style_transfer_tasks[run_id].update({
            "status": "completed",
            "summary": process_log,
            "result": {
                "results": final_results,
                "suggestions": suggestions
            }
        })

    except Exception as e:
        logging.error(f"Run ID {run_id}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        process_log.append(f"âŒ FATAL_ERROR: {e}")
        style_transfer_tasks[run_id].update({"status": "failed", "summary": process_log})