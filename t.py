import arxiv
import csv
import time
import os
import asyncio
import logging
import re
from datetime import datetime, date
from pathlib import Path
import shutil
from openai import AsyncOpenAI

# å‡è®¾æ‚¨çš„ config.py æ–‡ä»¶åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹
from config import (
    background_tasks, outputs_dir, workspace_dir,
    OPENAI_API_KEY, OPENAI_API_BASE, MODEL_NAME,
    MAX_CONCURRENT_TRANSLATIONS
)

def build_arxiv_query(keyword_phrase: str) -> str:
    """
    ä¸ºç»™å®šçš„å…³é”®è¯çŸ­è¯­æ„å»ºä¸€ä¸ªé«˜çº§æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ŒåŒæ—¶æœç´¢æ‘˜è¦(abs)å’Œæ ‡é¢˜(ti)ã€‚
    - å¯¹äºç‰¹æ®Šé…ç½®çš„çŸ­è¯­ï¼Œä½¿ç”¨é¢„è®¾çš„ AND/OR ç»„åˆã€‚
    - å¯¹äºå…¶ä»–çŸ­è¯­ï¼Œå°†æ‰€æœ‰å•è¯ç”¨ AND è¿æ¥ï¼Œæ¯ä¸ªå•è¯éƒ½åŒæ—¶åŒ¹é…æ ‡é¢˜æˆ–æ‘˜è¦ã€‚
    """
    _ABS_OR_TI_LLM = '((abs:LLM OR ti:LLM) OR (abs:"Large Language Model" OR ti:"Large Language Model"))'
    _ABS_OR_TI_RL = '((abs:RL OR ti:RL) OR (abs:"Reinforcement Learning" OR ti:"Reinforcement Learning"))'
    
    _SPECIAL_PHRASE_CONFIG = {
        "large language model agent rl": [_ABS_OR_TI_LLM, '(abs:agent OR ti:agent)', _ABS_OR_TI_RL],
        "llm rft": [_ABS_OR_TI_LLM, '(abs:RFT OR ti:RFT)'],
        "llm reinforcement learning finetuning": [_ABS_OR_TI_LLM, _ABS_OR_TI_RL, '(abs:Finetuning OR ti:Finetuning)'],
        "large language model rl": [_ABS_OR_TI_LLM, _ABS_OR_TI_RL]
    }
    
    phrase_lower = keyword_phrase.lower().strip()
    if phrase_lower in _SPECIAL_PHRASE_CONFIG:
        query_parts = _SPECIAL_PHRASE_CONFIG[phrase_lower]
        return f"({' AND '.join(query_parts)})"
    else:
        words = [word for word in keyword_phrase.split() if word]
        if not words:
            return ""
        
        query_parts = [f'(abs:{word} OR ti:{word})' for word in words]
        return f"({' AND '.join(query_parts)})"

def search_arxiv_by_date_range(keywords, start_date_str, end_date_str, max_results, process_log):
    """æ ¹æ®æ—¥æœŸèŒƒå›´ä» arXiv æ£€ç´¢è®ºæ–‡ï¼Œå¹¶ä¸ºé•¿å…³é”®è¯ç»„æ‰§è¡Œè¡¥å……æœç´¢ã€‚"""
    unique_papers = {}
    try:
        filter_start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        filter_end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    except (ValueError, TypeError):
        raise ValueError(f"æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ YYYY-MM-DDã€‚")

    if filter_start_date > filter_end_date:
        raise ValueError("èµ·å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸã€‚")

    process_log.append(f"INFO: å¼€å§‹æ£€ç´¢ï¼Œæ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")
    logging.info(f"å¼€å§‹æ£€ç´¢ï¼Œæ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")

    def _perform_search(query_keyword, original_keyword_for_result):
        advanced_query = build_arxiv_query(query_keyword)
        if not advanced_query:
            process_log.append(f"INFO: æŸ¥è¯¢å…³é”®è¯ '{query_keyword}' ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            return
            
        log_message_prefix = "è¡¥å……" if query_keyword != original_keyword_for_result else ""
        process_log.append(f"INFO: æ­£åœ¨æ‰§è¡Œ{log_message_prefix}æœç´¢ '{query_keyword}' (æŸ¥è¯¢: {advanced_query})")
        logging.info(f"æ­£åœ¨æ‰§è¡Œ{log_message_prefix}æœç´¢ '{query_keyword}' (æŸ¥è¯¢: {advanced_query})")
        
        try:
            search = arxiv.Search(
                query=advanced_query,
                max_results=max_results * 2,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            retrieved_count = 0
            for result in search.results():
                if retrieved_count >= max_results: break
                paper_date = result.published.date()
                if filter_start_date <= paper_date <= filter_end_date:
                    if result.entry_id not in unique_papers:
                        unique_papers[result.entry_id] = {
                            "title": result.title,
                            "published_date": paper_date.strftime("%Y-%m-%d"),
                            "summary_en": result.summary.replace("\n", " "),
                            "authors": [author.name for author in result.authors],
                            "arxiv_link": result.entry_id,
                            "pdf_link": result.pdf_url,
                            "original_keyword": original_keyword_for_result
                        }
                        retrieved_count += 1
            process_log.append(f"SUCCESS: {log_message_prefix}æœç´¢ '{query_keyword}' æ‰¾åˆ° {retrieved_count} ç¯‡æ–°è®ºæ–‡ã€‚")
        except Exception as e:
            logging.error(f"æœç´¢å…³é”®è¯ '{query_keyword}' æ—¶å‡ºé”™: {e}")
            process_log.append(f"WARNING: æœç´¢å…³é”®è¯ '{query_keyword}' æ—¶å‡ºé”™: {e}")

    for keyword in keywords:
        _perform_search(keyword, keyword)
        time.sleep(3)

        keyword_parts = keyword.split()
        if len(keyword_parts) > 3:
            supplementary_keyword = " ".join(keyword_parts[:-1])
            process_log.append(f"INFO: å…³é”®è¯ '{keyword}' åŒ…å«è¶…è¿‡3ä¸ªè¯ï¼Œå°†æ‰§è¡Œä¸€æ¬¡è¡¥å……æœç´¢ã€‚")
            
            _perform_search(supplementary_keyword, keyword)
            time.sleep(3)

    total_found = len(unique_papers)
    process_log.append(f"SUCCESS: æ‰€æœ‰å…³é”®è¯æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_found} ç¯‡ä¸é‡å¤çš„è®ºæ–‡ã€‚")
    logging.info(f"æ‰€æœ‰å…³é”®è¯æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_found} ç¯‡ä¸é‡å¤çš„è®ºæ–‡ã€‚")
    return list(unique_papers.values())


async def translate_one_abstract(aclient, abstract_en, target_language, semaphore):
    """ä½¿ç”¨ LLM å¼‚æ­¥ç¿»è¯‘å•ä¸ªæ‘˜è¦ã€‚"""
    if not abstract_en or not abstract_en.strip(): return ""
    async with semaphore:
        try:
            prompt_content = f"Please translate the following academic abstract into {target_language}. Keep the original formatting and technical terms. Abstract: "
            response = await aclient.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": prompt_content},
                    {"role": "user", "content": abstract_en}
                ],
                temperature=0.2,
                max_tokens=4000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"ç¿»è¯‘æ‘˜è¦æ—¶å‡ºé”™: {e}")
            return f"ç¿»è¯‘é”™è¯¯: {e}"

def sanitize_filename_part(text: str) -> str:
    """æ¸…ç†å­—ç¬¦ä¸²ï¼Œä½¿å…¶å¯å®‰å…¨åœ°ç”¨äºæ–‡ä»¶åã€‚"""
    text = re.sub(r'[\s,]+', '_', text)
    text = re.sub(r'[^\w\-_.]', '', text)
    return text[:50]

async def run_arxiv_search_and_process(run_id: str, request_params: dict):
    """åå°ä»»åŠ¡çš„ä¸»æ‰§è¡Œå‡½æ•°ï¼šæœç´¢ã€ç¿»è¯‘ã€ä¿å­˜ã€‚"""
    process_log = background_tasks[run_id]['summary']
    work_dir = workspace_dir / f"work_dir_{run_id}"
    work_dir.mkdir(exist_ok=True)
    
    try:
        papers = search_arxiv_by_date_range(
            keywords=request_params['keywords'],
            start_date_str=request_params['start_date'],
            end_date_str=request_params['end_date'],
            max_results=request_params['max_results'],
            process_log=process_log
        )
        background_tasks[run_id]['summary'] = process_log

        if not papers:
            process_log.append("INFO: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œä»»åŠ¡ç»“æŸã€‚")
            background_tasks[run_id].update({"status": "completed", "summary": process_log})
            return

        target_language = request_params.get('target_language')
        if target_language and target_language.strip():
            process_log.append(f"INFO: å¼€å§‹å°† {len(papers)} ç¯‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘ä¸º {target_language}...")
            background_tasks[run_id].update({"status": "translating", "summary": process_log})
            
            aclient = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TRANSLATIONS)
            
            translation_tasks = [
                translate_one_abstract(aclient, paper['summary_en'], target_language, semaphore)
                for paper in papers
            ]
            translated_summaries = await asyncio.gather(*translation_tasks, return_exceptions=True)
            
            for paper, translated in zip(papers, translated_summaries):
                paper['summary_translated'] = translated if not isinstance(translated, Exception) else f"ç¿»è¯‘å¤±è´¥: {translated}"
            process_log.append("SUCCESS: æ‰€æœ‰æ‘˜è¦ç¿»è¯‘å®Œæˆã€‚")
        else:
            process_log.append("INFO: æ— éœ€ç¿»è¯‘ã€‚")
            for paper in papers:
                paper['summary_translated'] = ""

        topic_str = sanitize_filename_part("_".join(request_params['keywords']))
        lang_str = sanitize_filename_part(target_language) if target_language else "en"
        start_str = request_params['start_date']
        end_str = request_params['end_date']
        count = len(papers)
        csv_filename = f"arxiv_papers_{topic_str}_{start_str}_to_{end_str}_{lang_str}_{count}.csv"
        output_path = outputs_dir / csv_filename
        
        process_log.append(f"INFO: å‡†å¤‡å°†ç»“æœå†™å…¥æ–‡ä»¶: {csv_filename}")
        
        fieldnames = ["åŸå§‹å…³é”®è¯", "è®ºæ–‡æ ‡é¢˜", "å‘è¡¨æ—¥æœŸ", "è‹±æ–‡æ‘˜è¦", f"ç¿»è¯‘æ‘˜è¦ ({lang_str})", "ä½œè€…åˆ—è¡¨", "arxivé“¾æ¥", "PDFé“¾æ¥"]
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for paper in papers:
                writer.writerow({
                    "åŸå§‹å…³é”®è¯": paper.get("original_keyword", "N/A"),
                    "è®ºæ–‡æ ‡é¢˜": paper['title'],
                    "å‘è¡¨æ—¥æœŸ": paper['published_date'],
                    "è‹±æ–‡æ‘˜è¦": paper['summary_en'],
                    f"ç¿»è¯‘æ‘˜è¦ ({lang_str})": paper['summary_translated'],
                    "ä½œè€…åˆ—è¡¨": ", ".join(paper['authors']),
                    "arxivé“¾æ¥": paper['arxiv_link'],
                    "PDFé“¾æ¥": paper.get('pdf_link', 'N/A')
                })
        
        process_log.append("ğŸ‰ SUCCESS: ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        background_tasks[run_id].update({
            "status": "completed",
            "summary": process_log,
            "output_path": str(output_path)
        })

    except Exception as e:
        logging.error(f"Run ID {run_id}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        process_log.append(f"âŒ FATAL_ERROR: {e}")
        background_tasks[run_id].update({"status": "failed", "summary": process_log})
    finally:
        if work_dir.exists():
            shutil.rmtree(work_dir)
            logging.info(f"Run ID {run_id}: å·²æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•ã€‚")