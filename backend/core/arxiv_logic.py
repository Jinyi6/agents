import arxiv
import csv
import time
import os
import asyncio
import logging
import re
from datetime import datetime, date
from pathlib import Path
import shutil
from openai import AsyncOpenAI

from config import (
    background_tasks, outputs_dir, workspace_dir,
    OPENAI_API_KEY, OPENAI_API_BASE, MODEL_NAME,
    MAX_CONCURRENT_TRANSLATIONS
)

def build_advanced_query_refactored(keyword_phrase: str) -> str:
    """ä¸ºç»™å®šçš„å…³é”®è¯çŸ­è¯­æ„å»ºä¸€ä¸ªä»…é’ˆå¯¹ arXiv æ‘˜è¦ (abs) çš„é«˜çº§æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚"""
    # [ä¿®æ”¹ç‚¹ 0]ï¼šå°†å…·ä½“çš„ prompt å†…å®¹æ›¿æ¢ä¸º "..."
    _ABS_LLM = '(abs:LLM OR abs:"Large Language Model")'
    _ABS_RL = '(abs:RL OR abs:"Reinforcement Learning")'
    _SPECIAL_PHRASE_CONFIG = {
        "large language model agent rl": [_ABS_LLM, 'abs:agent', _ABS_RL],
        "llm rft": [_ABS_LLM, 'abs:RFT'],
        "llm reinforcement learning finetuning": [_ABS_LLM, _ABS_RL, 'abs:Finetuning'],
        "large language model rl": [_ABS_LLM, _ABS_RL]
    }
    phrase_lower = keyword_phrase.lower()
    if phrase_lower in _SPECIAL_PHRASE_CONFIG:
        abs_parts = _SPECIAL_PHRASE_CONFIG[phrase_lower]
        return f"({' AND '.join(abs_parts)})"
    else:
        escaped_phrase = keyword_phrase.replace('"', '\\"')
        return f'(abs:"{escaped_phrase}")'

def search_arxiv_by_date_range(keywords, start_date_str, end_date_str, max_results, process_log):
    """æ ¹æ®æ—¥æœŸèŒƒå›´ä» arXiv æ£€ç´¢è®ºæ–‡ã€‚"""
    unique_papers = {}
    try:
        filter_start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        filter_end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    except (ValueError, TypeError):
        raise ValueError(f"æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ YYYY-MM-DDã€‚")

    if filter_start_date > filter_end_date:
        raise ValueError("èµ·å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸã€‚")

    process_log.append(f"INFO: å¼€å§‹æ£€ç´¢ï¼Œæ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")
    logging.info(f"å¼€å§‹æ£€ç´¢ï¼Œæ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")

    for keyword in keywords:
        advanced_query = build_advanced_query_refactored(keyword)
        process_log.append(f"INFO: æ­£åœ¨æœç´¢å…³é”®è¯ '{keyword}' (æŸ¥è¯¢: {advanced_query})")
        logging.info(f"æ­£åœ¨æœç´¢å…³é”®è¯ '{keyword}' (æŸ¥è¯¢: {advanced_query})")
        try:
            search = arxiv.Search(
                query=advanced_query,
                max_results=max_results * 2,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            retrieved_count = 0
            for result in search.results():
                if retrieved_count >= max_results: break
                paper_date = result.published.date()
                if filter_start_date <= paper_date <= filter_end_date:
                    if result.entry_id not in unique_papers:
                        unique_papers[result.entry_id] = {
                            "title": result.title,
                            "published_date": paper_date.strftime("%Y-%m-%d"),
                            "summary_en": result.summary.replace("\n", " "),
                            "authors": [author.name for author in result.authors],
                            "arxiv_link": result.entry_id,
                            "pdf_link": result.pdf_url,
                            "original_keyword": keyword
                        }
                        retrieved_count += 1
            process_log.append(f"SUCCESS: å…³é”®è¯ '{keyword}' æ‰¾åˆ° {retrieved_count} ç¯‡æ–°è®ºæ–‡ã€‚")
            time.sleep(3) # Be nice to arXiv API
        except Exception as e:
            logging.error(f"æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
            process_log.append(f"WARNING: æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
            continue

    total_found = len(unique_papers)
    process_log.append(f"SUCCESS: æ‰€æœ‰å…³é”®è¯æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_found} ç¯‡ä¸é‡å¤çš„è®ºæ–‡ã€‚")
    logging.info(f"æ‰€æœ‰å…³é”®è¯æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_found} ç¯‡ä¸é‡å¤çš„è®ºæ–‡ã€‚")
    return list(unique_papers.values())

async def translate_one_abstract(aclient, abstract_en, target_language, semaphore):
    """ä½¿ç”¨ LLM å¼‚æ­¥ç¿»è¯‘å•ä¸ªæ‘˜è¦ã€‚"""
    if not abstract_en or not abstract_en.strip(): return ""
    async with semaphore:
        try:
            # [ä¿®æ”¹ç‚¹ 0]ï¼šå°†å…·ä½“çš„ prompt å†…å®¹æ›¿æ¢ä¸º "..."
            prompt_content = "..."
            response = await aclient.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": prompt_content},
                    {"role": "user", "content": abstract_en}
                ],
                temperature=0.2,
                max_tokens=4000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"ç¿»è¯‘æ‘˜è¦æ—¶å‡ºé”™: {e}")
            return f"ç¿»è¯‘é”™è¯¯: {e}"

def sanitize_filename_part(text: str) -> str:
    """æ¸…ç†å­—ç¬¦ä¸²ï¼Œä½¿å…¶å¯å®‰å…¨åœ°ç”¨äºæ–‡ä»¶åã€‚"""
    text = re.sub(r'[\s,]+', '_', text) # Replace spaces and commas with underscore
    text = re.sub(r'[^\w\-_.]', '', text) # Remove all non-word, non-hyphen, non-underscore, non-dot characters
    return text[:50] # Truncate to a reasonable length

async def run_arxiv_search_and_process(run_id: str, request_params: dict):
    """åå°ä»»åŠ¡çš„ä¸»æ‰§è¡Œå‡½æ•°ï¼šæœç´¢ã€ç¿»è¯‘ã€ä¿å­˜ã€‚"""
    process_log = background_tasks[run_id]['summary']
    work_dir = workspace_dir / f"work_dir_{run_id}"
    work_dir.mkdir(exist_ok=True)
    
    try:
        papers = search_arxiv_by_date_range(
            keywords=request_params['keywords'],
            start_date_str=request_params['start_date'],
            end_date_str=request_params['end_date'],
            max_results=request_params['max_results'],
            process_log=process_log
        )
        background_tasks[run_id]['summary'] = process_log

        if not papers:
            process_log.append("INFO: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œä»»åŠ¡ç»“æŸã€‚")
            background_tasks[run_id].update({"status": "completed", "summary": process_log})
            return

        target_language = request_params.get('target_language')
        if target_language and target_language.strip():
            process_log.append(f"INFO: å¼€å§‹å°† {len(papers)} ç¯‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘ä¸º {target_language}...")
            background_tasks[run_id].update({"status": "translating", "summary": process_log})
            
            aclient = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TRANSLATIONS)
            
            translation_tasks = [
                translate_one_abstract(aclient, paper['summary_en'], target_language, semaphore)
                for paper in papers
            ]
            translated_summaries = await asyncio.gather(*tasks, return_exceptions=True)
            
            for paper, translated in zip(papers, translated_summaries):
                paper['summary_translated'] = translated if not isinstance(translated, Exception) else f"ç¿»è¯‘å¤±è´¥: {translated}"
            process_log.append("SUCCESS: æ‰€æœ‰æ‘˜è¦ç¿»è¯‘å®Œæˆã€‚")
        else:
            process_log.append("INFO: æ— éœ€ç¿»è¯‘ã€‚")
            for paper in papers:
                paper['summary_translated'] = "" # Add empty column for consistency

        # ç”ŸæˆåŠ¨æ€æ–‡ä»¶å
        topic_str = sanitize_filename_part("_".join(request_params['keywords']))
        lang_str = sanitize_filename_part(target_language) if target_language else "en"
        start_str = request_params['start_date']
        end_str = request_params['end_date']
        count = len(papers)
        csv_filename = f"arxiv_papers_{topic_str}_{start_str}_to_{end_str}_{lang_str}_{count}.csv"
        output_path = outputs_dir / csv_filename
        
        process_log.append(f"INFO: å‡†å¤‡å°†ç»“æœå†™å…¥æ–‡ä»¶: {csv_filename}")
        
        fieldnames = ["åŸå§‹å…³é”®è¯", "è®ºæ–‡æ ‡é¢˜", "å‘è¡¨æ—¥æœŸ", "è‹±æ–‡æ‘˜è¦", f"ç¿»è¯‘æ‘˜è¦ ({lang_str})", "ä½œè€…åˆ—è¡¨", "arxivé“¾æ¥", "PDFé“¾æ¥"]
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for paper in papers:
                writer.writerow({
                    "åŸå§‹å…³é”®è¯": paper.get("original_keyword", "N/A"),
                    "è®ºæ–‡æ ‡é¢˜": paper['title'],
                    "å‘è¡¨æ—¥æœŸ": paper['published_date'],
                    "è‹±æ–‡æ‘˜è¦": paper['summary_en'],
                    f"ç¿»è¯‘æ‘˜è¦ ({lang_str})": paper['summary_translated'],
                    "ä½œè€…åˆ—è¡¨": ", ".join(paper['authors']),
                    "arxivé“¾æ¥": paper['arxiv_link'],
                    "PDFé“¾æ¥": paper.get('pdf_link', 'N/A')
                })
        
        process_log.append("ğŸ‰ SUCCESS: ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        background_tasks[run_id].update({
            "status": "completed",
            "summary": process_log,
            "output_path": str(output_path)
        })

    except Exception as e:
        logging.error(f"Run ID {run_id}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        process_log.append(f"âŒ FATAL_ERROR: {e}")
        background_tasks[run_id].update({"status": "failed", "summary": process_log})
    finally:
        # æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•
        if work_dir.exists():
            shutil.rmtree(work_dir)
            logging.info(f"Run ID {run_id}: å·²æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•ã€‚")
