import arxiv
import csv
import time
import os
import asyncio
import logging
import re
import math
import random
from datetime import datetime, date
from pathlib import Path
import shutil
from openai import AsyncOpenAI

# å‡è®¾æ‚¨çš„ config.py æ–‡ä»¶åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹
from config import (
    background_tasks, outputs_dir, workspace_dir,
    OPENAI_API_KEY, OPENAI_API_BASE, MODEL_NAME,
    MAX_CONCURRENT_TRANSLATIONS
)

def build_arxiv_query(keyword_phrase: str) -> str:
    """
    ä¸ºç»™å®šçš„å…³é”®è¯çŸ­è¯­æ„å»ºä¸€ä¸ªé«˜çº§æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ŒåŒæ—¶æœç´¢æ‘˜è¦(abs)å’Œæ ‡é¢˜(ti)ã€‚
    - å¯¹äºç‰¹æ®Šé…ç½®çš„çŸ­è¯­ï¼Œä½¿ç”¨é¢„è®¾çš„ AND/OR ç»„åˆã€‚
    - å¯¹äºå…¶ä»–çŸ­è¯­ï¼Œå°†æ‰€æœ‰å•è¯ç”¨ AND è¿æ¥ï¼Œæ¯ä¸ªå•è¯éƒ½åŒæ—¶åŒ¹é…æ ‡é¢˜æˆ–æ‘˜è¦ã€‚
    """
    _ABS_OR_TI_LLM = '((abs:LLM OR ti:LLM) OR (abs:"Large Language Model" OR ti:"Large Language Model"))'
    _ABS_OR_TI_RL = '((abs:RL OR ti:RL) OR (abs:"Reinforcement Learning" OR ti:"Reinforcement Learning"))'
    
    _SPECIAL_PHRASE_CONFIG = {
        "large language model agent rl": [_ABS_OR_TI_LLM, '(abs:agent OR ti:agent)', _ABS_OR_TI_RL],
        "llm rft": [_ABS_OR_TI_LLM, '(abs:RFT OR ti:RFT)'],
        "llm reinforcement learning finetuning": [_ABS_OR_TI_LLM, _ABS_OR_TI_RL, '(abs:Finetuning OR ti:Finetuning)'],
        "large language model rl": [_ABS_OR_TI_LLM, _ABS_OR_TI_RL]
    }
    
    phrase_lower = keyword_phrase.lower().strip()
    if phrase_lower in _SPECIAL_PHRASE_CONFIG:
        query_parts = _SPECIAL_PHRASE_CONFIG[phrase_lower]
        return f"({' AND '.join(query_parts)})"
    else:
        words = [word for word in keyword_phrase.split() if word]
        if not words:
            return ""
        
        query_parts = [f'(abs:{word} OR ti:{word})' for word in words]
        return f"({' AND '.join(query_parts)})"

def search_arxiv_by_date_range(keywords, start_date_str, end_date_str, max_results, process_log):
    """
    æ ¹æ®æ—¥æœŸèŒƒå›´ä»arXivæ£€ç´¢è®ºæ–‡ï¼Œé‡‡ç”¨åˆ†å±‚é™çº§æœç´¢ç­–ç•¥ã€‚
    'keywords' å‚æ•°æ˜¯ä¸€ä¸ªæŒ‰ä¼˜å…ˆçº§æ’åºçš„å…³é”®è¯åˆ—è¡¨ã€‚
    """
    unique_papers = {}
    try:
        filter_start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        filter_end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    except (ValueError, TypeError):
        raise ValueError(f"æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ YYYY-MM-DDã€‚")

    if filter_start_date > filter_end_date:
        raise ValueError("èµ·å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸã€‚")

    process_log.append(f"INFO: å¼€å§‹æ£€ç´¢ï¼Œæ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")
    logging.info(f"å¼€å§‹æ£€ç´¢ï¼Œæ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")

    def _perform_search(query_keyword, original_keyword_for_result, stage_name):
        advanced_query = build_arxiv_query(query_keyword)
        if not advanced_query:
            process_log.append(f"INFO: ({stage_name}) æŸ¥è¯¢å…³é”®è¯ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            return
            
        process_log.append(f"INFO: ({stage_name}) æ­£åœ¨æ‰§è¡Œæœç´¢ '{query_keyword}' (æŸ¥è¯¢: {advanced_query})")
        logging.info(f"({stage_name}) æ­£åœ¨æ‰§è¡Œæœç´¢ '{query_keyword}' (æŸ¥è¯¢: {advanced_query})")
        
        try:
            search = arxiv.Search(
                query=advanced_query,
                max_results=max_results * 2,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            retrieved_count = 0
            for result in search.results():
                if retrieved_count >= max_results: break
                paper_date = result.published.date()
                if filter_start_date <= paper_date <= filter_end_date:
                    if result.entry_id not in unique_papers:
                        unique_papers[result.entry_id] = {
                            "title": result.title,
                            "published_date": paper_date.strftime("%Y-%m-%d"),
                            "summary_en": result.summary.replace("\n", " "),
                            "authors": [author.name for author in result.authors],
                            "arxiv_link": result.entry_id,
                            "pdf_link": result.pdf_url,
                            "original_keyword": original_keyword_for_result
                        }
                        retrieved_count += 1
            process_log.append(f"SUCCESS: ({stage_name}) æœç´¢ '{query_keyword}' æ‰¾åˆ° {retrieved_count} ç¯‡æ–°è®ºæ–‡ã€‚")
        except Exception as e:
            logging.error(f"æœç´¢å…³é”®è¯ '{query_keyword}' æ—¶å‡ºé”™: {e}")
            process_log.append(f"WARNING: ({stage_name}) æœç´¢ '{query_keyword}' æ—¶å‡ºé”™: {e}")

    # [æ ¸å¿ƒä¿®æ”¹]: ç”¨æ–°çš„åˆ†å±‚é€»è¾‘æ›¿æ¢æ—§çš„å¾ªç¯
    keyword_list = keywords
    num_keywords = len(keyword_list)
    original_query_str_for_log = ", ".join(keyword_list)

    if num_keywords == 0:
        process_log.append("WARNING: æœªæä¾›ä»»ä½•å…³é”®è¯ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
        return []

    # --- Stage 1: Primary search with all keywords ---
    full_query = " ".join(keyword_list)
    _perform_search(full_query, original_query_str_for_log, "ä¸»æœç´¢ (å…¨éƒ¨å…³é”®è¯)")
    time.sleep(3)

    # --- Stage 2 & 3: Supplementary searches if N > 3 ---
    if num_keywords > 3:
        # Stage 2: Search with top 80% of keywords
        num_top_80 = math.ceil(num_keywords * 0.8)
        if num_top_80 < num_keywords:
            top_80_keywords = keyword_list[:num_top_80]
            top_80_query = " ".join(top_80_keywords)
            _perform_search(top_80_query, original_query_str_for_log, "è¡¥å……æœç´¢ (å‰80%)")
            time.sleep(3)

        # Stage 3: Searches with top 60% + one from the bottom 40%
        num_top_60 = math.floor(num_keywords * 0.6)
        if num_top_60 < num_keywords and num_top_60 > 0:
            base_keywords = keyword_list[:num_top_60]
            optional_keywords = keyword_list[num_top_60:]
            
            stage_name = f"è¡¥å……æœç´¢ (å‰60%)"
            _perform_search(" ".join(base_keywords), original_query_str_for_log, stage_name)
            # Shuffle the optional keywords then iterate through them
            random.shuffle(optional_keywords) 

            for i, optional_keyword in enumerate(optional_keywords):
                combined_list = base_keywords + [optional_keyword]
                supplementary_query = " ".join(combined_list)
                stage_name = f"è¡¥å……æœç´¢ (å‰60% + å¯é€‰è¯ {i+1})"
                _perform_search(supplementary_query, original_query_str_for_log, stage_name)
                time.sleep(3)


    total_found = len(unique_papers)
    process_log.append(f"SUCCESS: æ‰€æœ‰åˆ†å±‚æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_found} ç¯‡ä¸é‡å¤çš„è®ºæ–‡ã€‚")
    logging.info(f"æ‰€æœ‰åˆ†å±‚æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_found} ç¯‡ä¸é‡å¤çš„è®ºæ–‡ã€‚")
    return list(unique_papers.values())


async def translate_one_abstract(aclient, abstract_en, target_language, semaphore):
    """ä½¿ç”¨ LLM å¼‚æ­¥ç¿»è¯‘å•ä¸ªæ‘˜è¦ã€‚"""
    if not abstract_en or not abstract_en.strip(): return ""
    async with semaphore:
        try:
            prompt_content = f"Please translate the following academic abstract into {target_language}. Keep the original formatting and technical terms. Abstract: "
            response = await aclient.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": prompt_content},
                    {"role": "user", "content": abstract_en}
                ],
                temperature=0.2,
                max_tokens=4000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"ç¿»è¯‘æ‘˜è¦æ—¶å‡ºé”™: {e}")
            return f"ç¿»è¯‘é”™è¯¯: {e}"

def sanitize_filename_part(text: str) -> str:
    """æ¸…ç†å­—ç¬¦ä¸²ï¼Œä½¿å…¶å¯å®‰å…¨åœ°ç”¨äºæ–‡ä»¶åã€‚"""
    text = re.sub(r'[\s,]+', '_', text)
    text = re.sub(r'[^\w\-_.]', '', text)
    return text[:50]

async def run_arxiv_search_and_process(run_id: str, request_params: dict):
    """åå°ä»»åŠ¡çš„ä¸»æ‰§è¡Œå‡½æ•°ï¼šæœç´¢ã€ç¿»è¯‘ã€ä¿å­˜ã€‚"""
    process_log = background_tasks[run_id]['summary']
    work_dir = workspace_dir / f"work_dir_{run_id}"
    work_dir.mkdir(exist_ok=True)
    
    try:
        papers = search_arxiv_by_date_range(
            keywords=request_params['keywords'],
            start_date_str=request_params['start_date'],
            end_date_str=request_params['end_date'],
            max_results=request_params['max_results'],
            process_log=process_log
        )
        background_tasks[run_id]['summary'] = process_log

        if not papers:
            process_log.append("INFO: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œä»»åŠ¡ç»“æŸã€‚")
            background_tasks[run_id].update({"status": "completed", "summary": process_log})
            return

        target_language = request_params.get('target_language')
        if target_language and target_language.strip():
            process_log.append(f"INFO: å¼€å§‹å°† {len(papers)} ç¯‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘ä¸º {target_language}...")
            background_tasks[run_id].update({"status": "translating", "summary": process_log})
            
            aclient = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_TRANSLATIONS)
            
            translation_tasks = [
                translate_one_abstract(aclient, paper['summary_en'], target_language, semaphore)
                for paper in papers
            ]
            translated_summaries = await asyncio.gather(*translation_tasks, return_exceptions=True)
            
            for paper, translated in zip(papers, translated_summaries):
                paper['summary_translated'] = translated if not isinstance(translated, Exception) else f"ç¿»è¯‘å¤±è´¥: {translated}"
            process_log.append("SUCCESS: æ‰€æœ‰æ‘˜è¦ç¿»è¯‘å®Œæˆã€‚")
        else:
            process_log.append("INFO: æ— éœ€ç¿»è¯‘ã€‚")
            for paper in papers:
                paper['summary_translated'] = ""

        topic_str = sanitize_filename_part("_".join(request_params['keywords']))
        lang_str = sanitize_filename_part(target_language) if target_language else "en"
        start_str = request_params['start_date']
        end_str = request_params['end_date']
        count = len(papers)
        csv_filename = f"arxiv_papers_{topic_str}_{start_str}_to_{end_str}_{lang_str}_{count}.csv"
        output_path = outputs_dir / csv_filename
        
        process_log.append(f"INFO: å‡†å¤‡å°†ç»“æœå†™å…¥æ–‡ä»¶: {csv_filename}")
        
        fieldnames = ["åŸå§‹å…³é”®è¯", "è®ºæ–‡æ ‡é¢˜", "å‘è¡¨æ—¥æœŸ", "è‹±æ–‡æ‘˜è¦", f"ç¿»è¯‘æ‘˜è¦ ({lang_str})", "ä½œè€…åˆ—è¡¨", "arxivé“¾æ¥", "PDFé“¾æ¥"]
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for paper in papers:
                writer.writerow({
                    "åŸå§‹å…³é”®è¯": paper.get("original_keyword", "N/A"),
                    "è®ºæ–‡æ ‡é¢˜": paper['title'],
                    "å‘è¡¨æ—¥æœŸ": paper['published_date'],
                    "è‹±æ–‡æ‘˜è¦": paper['summary_en'],
                    f"ç¿»è¯‘æ‘˜è¦ ({lang_str})": paper['summary_translated'],
                    "ä½œè€…åˆ—è¡¨": ", ".join(paper['authors']),
                    "arxivé“¾æ¥": paper['arxiv_link'],
                    "PDFé“¾æ¥": paper.get('pdf_link', 'N/A')
                })
        
        process_log.append("ğŸ‰ SUCCESS: ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        background_tasks[run_id].update({
            "status": "completed",
            "summary": process_log,
            "output_path": str(output_path)
        })

    except Exception as e:
        logging.error(f"Run ID {run_id}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        process_log.append(f"âŒ FATAL_ERROR: {e}")
        background_tasks[run_id].update({"status": "failed", "summary": process_log})
    finally:
        if work_dir.exists():
            shutil.rmtree(work_dir)
            logging.info(f"Run ID {run_id}: å·²æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•ã€‚")
