import logging
import json
from typing import Dict, List, Optional

from openai import OpenAI

from config import (
    OPENAI_API_KEY, OPENAI_API_BASE, MODEL_NAME, style_transfer_tasks
)
from core.utils import retry_step

# --- åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ ---
client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)

def build_prompt(original_text: str, must_include_keywords: Optional[List[str]], reference_keywords: Optional[List[str]], style_requirements: Optional[List[str]], style_example: Optional[str], previous_results: Optional[List[str]] = None) -> str:
    """æ„å»ºç”¨äºæ–‡æœ¬æ¶¦è‰²çš„è¯¦ç»†æç¤ºè¯"""
    
    prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šç‰¹å®šä¸“ä¸šé¢†åŸŸçš„å­¦æœ¯å†™ä½œä¸“å®¶å’Œé«˜çº§æ–‡æœ¬ç¼–è¾‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸€ç³»åˆ—ç²¾ç¡®çš„æŒ‡ä»¤ï¼Œå¯¹ä¸€æ®µåˆå§‹æ–‡æœ¬è¿›è¡Œæ·±åº¦ã€ä¸“ä¸šçš„é‡æ„å’Œä¼˜åŒ–ã€‚\n\n"
    prompt += "ã€æ ¸å¿ƒä»»åŠ¡ã€‘\n"
    prompt += "æ ¹æ®ä¸‹è¿°çš„è¾“å…¥ææ–™å’Œå…·ä½“è¦æ±‚ï¼Œå°†**[å¾…æ”¹å†™çš„è¡¨è¿°]**æ”¹å†™ä¸ºä¸€æ®µåœ¨å†…å®¹ä¸Šå®Œæ•´ã€å‡†ç¡®ï¼Œåœ¨é£æ ¼ä¸Šé«˜åº¦ä¸“ä¸šã€æ­£å¼ã€ä¸¥è°¨çš„æ–‡æœ¬ã€‚\n\n"

    prompt += """
ã€æ”¹å†™æ‰§è¡ŒæŒ‡ä»¤ã€‘
1.  **å¿ å®äºåŸæ„**: ç¡®ä¿æ”¹å†™åçš„æ–‡æœ¬å…¨é¢ã€æ— é—æ¼åœ°è¦†ç›–**[å¾…æ”¹å†™çš„è¡¨è¿°]**ä¸­çš„æ‰€æœ‰æ ¸å¿ƒä¿¡æ¯ç‚¹å’Œé€»è¾‘å…³ç³»ã€‚
2.  **æ·±åº¦é£æ ¼åˆ†æ**: åœ¨åŠ¨ç¬”å‰ï¼Œè¯·å¯¹**[é£æ ¼å‚è€ƒç¤ºä¾‹]**è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼Œé‡ç‚¹å…³æ³¨ä»¥ä¸‹ç»´åº¦ï¼š
    * **é˜è¿°è§†è§’ (Perspective)**: åˆ†æèŒƒä¾‹æ˜¯ä»ä½•ç§ç«‹åœºè¿›è¡Œé˜è¿°çš„ï¼ˆä¾‹å¦‚ï¼šå®¢è§‚ç ”ç©¶è€…ã€æŠ€æœ¯æŠ¥å‘Šæ’°å†™è€…ã€é¢†åŸŸè¯„è®ºå‘˜ç­‰ï¼‰ã€‚
    * **å¥å¼ç»“æ„ (Sentence Structure)**: è§£æ„èŒƒä¾‹ä¸­å¥å­çš„å…¸å‹é•¿åº¦ã€å¤æ‚æ€§ï¼ˆå¦‚ä¸»ä»å¤åˆå¥çš„ä½¿ç”¨ï¼‰å’Œè¯­æ³•èŒƒå¼ã€‚
    * **è¯ç»„ä¸æ­é… (Phrasing & Collocation)**: è¯†åˆ«èŒƒä¾‹ä¸­å…·æœ‰æ ‡å¿—æ€§çš„ä¸“ä¸šæœ¯è¯­ã€å›ºå®šçŸ­è¯­æ­é…å’Œè¡Œæ–‡èŠ‚å¥ã€‚
    
    **ç³»ç»Ÿæ€§é£æ ¼å¤ç°**: åœ¨æ”¹å†™è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»ä¸»åŠ¨ã€ç³»ç»Ÿåœ°è¿ç”¨ä½ åœ¨ä¸Šä¸€æ­¥ä¸­åˆ†æå‡ºçš„é£æ ¼ç‰¹å¾ã€‚ä½ çš„æœ€ç»ˆè¾“å‡ºåœ¨**é˜è¿°è§†è§’ã€å¥å¼ç»“æ„ã€è¯ç»„æ­é…**ä¸Šï¼Œéƒ½åº”ä¸**[é£æ ¼å‚è€ƒç¤ºä¾‹]**ä¿æŒé«˜åº¦ä¸€è‡´æ€§ã€‚
"""
    prompt += "---ã€è¾“å…¥ææ–™æ¸…å•ã€‘---\n"
    prompt += f"1. **[å¾…æ”¹å†™çš„è¡¨è¿°]**:\n   - {original_text}\n\n"
    
    if must_include_keywords:
        prompt += f"2. **[å¿…é¡»åŒ…å«çš„å…³é”®è¯]** (å¿…é¡»ä¸€å­—ä¸å·®åœ°ã€è‡ªç„¶åœ°åµŒå…¥åˆ°æ”¹å†™åçš„æ–‡æœ¬ä¸­):\n"
        for keyword in must_include_keywords:
            prompt += f"   - `{keyword}`\n"
        prompt += "\n"

    if reference_keywords:
        prompt += f"3. **[éœ€è¡¨è¾¾å«ä¹‰çš„å…³é”®è¯]** (ä¸å¿…ç›´æ¥ä½¿ç”¨åŸè¯ï¼Œä½†å¿…é¡»å‡†ç¡®ã€å®Œæ•´åœ°ä¼ è¾¾å…¶æ ¸å¿ƒè¯­ä¹‰):\n"
        for keyword in reference_keywords:
            prompt += f"   - {keyword}\n"
        prompt += "\n"

    if style_requirements:
        prompt += f"4. **[é£æ ¼è¦æ±‚]**:\n"
        for style in style_requirements:
            prompt += f"   - {style}\n"
        prompt += "\n"

    if style_example:
        prompt += f"5. **[é£æ ¼å‚è€ƒç¤ºä¾‹]** (è¯·æ·±åº¦åˆ†æå¹¶æ¨¡ä»¿å…¶é˜è¿°è§†è§’ã€å¥å¼ç»“æ„å’Œè¯ç»„æ­é…):\n"
        prompt += f"   - \"{style_example}\"\n\n"
        
    prompt += "---ã€æ”¹å†™æ‰§è¡ŒæŒ‡ä»¤ã€‘---\n"
    
    if previous_results:
        prompt += "ä½ ä¹‹å‰å·²ç»ç”Ÿæˆäº†ä»¥ä¸‹ç‰ˆæœ¬ï¼Œè¯·åœ¨æœ¬æ¬¡ç”Ÿæˆä¸­æä¾›ä¸€ä¸ªä¸ä¹‹å‰ç‰ˆæœ¬**æˆªç„¶ä¸åŒ**çš„ã€å…¨æ–°çš„ã€åˆ›æ–°çš„ç‰ˆæœ¬ã€‚\n"
        prompt += "[ä¹‹å‰å·²ç”Ÿæˆçš„ç‰ˆæœ¬]:\n"
        for i, result in enumerate(previous_results):
            prompt += f"{i+1}. {result}\n"
        prompt += "\n"
        prompt += "è¯·ä¸¥æ ¼éµå¾ªä¸Šè¿°æ‰€æœ‰è¦æ±‚ï¼Œåªè¾“å‡ºä¸€ä¸ª**æ–°**çš„ã€ç»è¿‡æ¶¦è‰²çš„æ–‡æœ¬ç‰ˆæœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚"
    else:
        num_results = "3åˆ°5ä¸ª"
        prompt += f"è¯·ä¸¥æ ¼éµå¾ªä¸Šè¿°æ‰€æœ‰è¦æ±‚ï¼Œç”Ÿæˆ **{num_results}** ä¸ªç»è¿‡æ¶¦è‰²çš„ã€é£æ ¼å„å¼‚çš„æ–‡æœ¬ç‰ˆæœ¬ã€‚è¯·ä»¥JSONæ ¼å¼çš„åˆ—è¡¨å½¢å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š[\"ç»“æœ1\", \"ç»“æœ2\", ...]ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚"

    return prompt

@retry_step
def call_llm_for_style_transfer(prompt: str, is_json: bool = False) -> any:
    """è°ƒç”¨LLMè¿›è¡Œé£æ ¼è½¬æ¢ï¼Œå¹¶æ ¹æ®éœ€è¦è§£æJSON"""
    logging.info("æ­£åœ¨ä¸ LLM äº¤äº’è¿›è¡Œæ–‡æœ¬æ¶¦è‰²...")
    
    response_format = {"type": "json_object"} if is_json else {"type": "text"}

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.4, # ç¨å¾®æé«˜æ¸©åº¦ä»¥è·å¾—æ›´å¤šæ ·åŒ–çš„ç»“æœ
        response_format=response_format
    )
    
    content = response.choices[0].message.content.strip()
    logging.info(f"LLM Response (raw): {content[:500]}...")

    if is_json:
        try:
            # å…¼å®¹å¤„ç†LLMå¯èƒ½è¿”å›çš„è¢«```json ... ```åŒ…è£¹çš„å“åº”
            if content.startswith("```json"):
                content = content[7:-3].strip()
            return json.loads(content)
        except json.JSONDecodeError as e:
            logging.error(f"LLM did not return valid JSON: {e}")
            raise ValueError("LLM è¿”å›äº†æ— æ•ˆçš„JSONæ ¼å¼ã€‚")
    else:
        return content


def run_style_transfer_logic(run_id: str, request_params: dict):
    """
    æ‰§è¡Œæ–‡æœ¬æ¶¦è‰²ä»»åŠ¡çš„ä¸»é€»è¾‘ã€‚
    """
    process_log = style_transfer_tasks[run_id]['summary']
    mode = request_params.get("mode", "æ ‡å‡†")
    
    try:
        if mode == "ä¸“ä¸š":
            process_log.append(f"INFO: å·²å¯åŠ¨ã€ä¸“ä¸šæ¨¡å¼ã€‘ï¼Œå°†æ‰§è¡Œ 7+2 è½® LLM è°ƒç”¨ã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log
            
            # 1. è¿­ä»£ç”Ÿæˆ7ä¸ªç»“æœ
            generated_results = []
            for i in range(7):
                process_log.append(f"INFO: æ­£åœ¨è¿›è¡Œç¬¬ {i+1}/7 è½®è¿­ä»£ç”Ÿæˆ...")
                style_transfer_tasks[run_id]['summary'] = process_log
                
                prompt = build_prompt(previous_results=generated_results, **request_params)
                new_result = call_llm_for_style_transfer(prompt, is_json=False)
                generated_results.append(new_result)
                process_log.append(f"SUCCESS: ç¬¬ {i+1} è½®ç”Ÿæˆå®Œæˆã€‚")
                style_transfer_tasks[run_id]['summary'] = process_log

            # 2. LLMæŒ‘é€‰æœ€ä½³4ä¸ª
            process_log.append("INFO: 7è½®è¿­ä»£å®Œæˆï¼Œæ­£åœ¨è°ƒç”¨ LLM æŒ‘é€‰æœ€ä½³çš„4ä¸ªç»“æœ...")
            style_transfer_tasks[run_id]['summary'] = process_log
            
            selection_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ–‡æœ¬ç¼–è¾‘å’Œè¯„è®ºå®¶ã€‚è¿™é‡Œæœ‰7ä¸ªåŸºäºç›¸åŒè¦æ±‚æ¶¦è‰²åçš„æ–‡æœ¬ç‰ˆæœ¬ã€‚è¯·ä»”ç»†è¯„ä¼°å®ƒä»¬ï¼Œå¹¶é€‰å‡ºå…¶ä¸­**æœ€ä¼˜ç§€ã€æœ€ç¬¦åˆè¦æ±‚ã€ä¸”é£æ ¼å·®å¼‚æœ€æ˜æ˜¾**çš„4ä¸ªç‰ˆæœ¬ã€‚

[åŸå§‹è¦æ±‚]
- åŸå§‹è¡¨è¿°: {request_params['original_text']}
- å¿…é¡»åŒ…å«çš„å…³é”®è¯: {request_params.get('must_include_keywords')}
- é£æ ¼è¦æ±‚: {request_params.get('style_requirements')}
- é£æ ¼å‚è€ƒç¤ºä¾‹: {request_params.get('style_example')}

[7ä¸ªå€™é€‰ç‰ˆæœ¬]
{chr(10).join(f'{i+1}. {res}' for i, res in enumerate(generated_results))}

è¯·ä»¥JSONæ ¼å¼è¿”å›ä¸€ä¸ªåŒ…å«ä½ é€‰å‡ºçš„4ä¸ªæ–‡æœ¬çš„åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼š["é€‰æ‹©çš„ç‰ˆæœ¬A", "é€‰æ‹©çš„ç‰ˆæœ¬B", ...]ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚
"""
            final_results = call_llm_for_style_transfer(selection_prompt, is_json=True)
            if not isinstance(final_results, list) or len(final_results) != 4:
                # å¦‚æœLLMè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ™æ‰‹åŠ¨é€‰æ‹©å‰4ä¸ªä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
                logging.warning("LLMæœªèƒ½æ­£ç¡®æŒ‘é€‰4ä¸ªç»“æœï¼Œå°†é»˜è®¤ä½¿ç”¨å‰4ä¸ªã€‚")
                final_results = generated_results[:4]

            process_log.append("SUCCESS: LLM å·²æŒ‘é€‰å‡º4ä¸ªæœ€ä½³ç»“æœã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log

        else: # æ ‡å‡†æ¨¡å¼
            process_log.append(f"INFO: å·²å¯åŠ¨ã€æ ‡å‡†æ¨¡å¼ã€‘ï¼Œå°†æ‰§è¡Œ 1+1 è½® LLM è°ƒç”¨ã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log
            
            prompt = build_prompt(**request_params)
            final_results = call_llm_for_style_transfer(prompt, is_json=True)
            if not isinstance(final_results, list):
                 raise ValueError("LLMæœªèƒ½è¿”å›ç»“æœåˆ—è¡¨ã€‚")

            process_log.append(f"SUCCESS: LLM å·²ç”Ÿæˆ {len(final_results)} æ¡æ¶¦è‰²ç»“æœã€‚")
            style_transfer_tasks[run_id]['summary'] = process_log
        
        # 3. LLMç”Ÿæˆä¿®æ”¹å»ºè®®
        process_log.append("INFO: æ­£åœ¨è°ƒç”¨ LLM ä¸ºæœ€ç»ˆç»“æœç”Ÿæˆä¿®æ”¹å»ºè®®...")
        style_transfer_tasks[run_id]['summary'] = process_log
        
        suggestions_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„å†™ä½œåŠ©ç†ã€‚è¿™é‡Œæœ‰å‡ æ¡ç”±AIæ¶¦è‰²åçš„æ–‡æœ¬ã€‚è¯·ä½ ç«™åœ¨ç”¨æˆ·çš„è§’åº¦ï¼Œæ£€æŸ¥è¿™äº›ç»“æœæ˜¯å¦å®Œå…¨ç¬¦åˆåŸå§‹è¦æ±‚ï¼Œå¹¶æä¾›ä¸€å°æ®µç²¾ç‚¼çš„ã€å¯æ“ä½œçš„ä¿®æ”¹å»ºè®®ã€‚

[åŸå§‹è¦æ±‚]
- åŸå§‹è¡¨è¿°: {request_params['original_text']}
- å¿…é¡»åŒ…å«çš„å…³é”®è¯: {request_params.get('must_include_keywords')}
- å‚è€ƒå…³é”®è¯: {request_params.get('reference_keywords')}
- é£æ ¼è¦æ±‚: {request_params.get('style_requirements')}
- é£æ ¼å‚è€ƒç¤ºä¾‹: {request_params.get('style_example')}

[æœ€ç»ˆæ¶¦è‰²ç»“æœ]
{chr(10).join(f'- {res}' for res in final_results)}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€å°æ®µæ–‡æœ¬æç¤ºï¼ŒæŒ‡å‡ºè¿™äº›ç»“æœä¸­å¯èƒ½å­˜åœ¨çš„ã€éœ€è¦ç”¨æˆ·æ‰‹åŠ¨å¾®è°ƒçš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šæŸä¸ªå¿…é¡»åŒ…å«çš„å…³é”®è¯æ˜¯å¦è‡ªç„¶èå…¥ï¼Ÿé£æ ¼æ˜¯å¦å®Œå…¨å¯¹é½ï¼Ÿï¼‰ï¼Œå¹¶ç»™å‡ºä¿®æ”¹å»ºè®®ã€‚ä½ çš„å›ç­”åº”è¯¥æ˜¯ç›´æ¥é¢å‘ç”¨æˆ·çš„ã€å‹å¥½çš„æ–‡æœ¬ã€‚
"""
        suggestions = call_llm_for_style_transfer(suggestions_prompt, is_json=False)
        process_log.append("SUCCESS: LLM å·²ç”Ÿæˆä¿®æ”¹å»ºè®®ã€‚")
        
        # 4. ä»»åŠ¡å®Œæˆ
        process_log.append("ğŸ‰ SUCCESS: æ–‡æœ¬æ¶¦è‰²ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        style_transfer_tasks[run_id].update({
            "status": "completed",
            "summary": process_log,
            "result": {
                "results": final_results,
                "suggestions": suggestions
            }
        })

    except Exception as e:
        logging.error(f"Run ID {run_id}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        process_log.append(f"âŒ FATAL_ERROR: {e}")
        style_transfer_tasks[run_id].update({"status": "failed", "summary": process_log})

